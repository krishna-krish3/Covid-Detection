{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9CZ35gVERDu5olNVqSLP/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna-krish3/Covid-Detection/blob/main/covid_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_XOhL8cNH-J"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb1gwgATNSIC",
        "outputId": "ff1cb6eb-ecc5-4cd4-cdee-5ea93871441d"
      },
      "source": [
        "#from cadence models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUXKFGxBNfwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2ceec014-60c1-4f5d-8d54-8930d0031286"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import helper\n",
        "import matplotlib.pyplot as plt\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.48, 0.45, 0.40], std=[0.23, 0.23, 0.23])])\n",
        "                                \n",
        "dataset = datasets.ImageFolder(\"/content/gdrive/MyDrive/Colab Notebooks/Covid_Images/\", transform = transform)\n",
        "bsize = len(dataset)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bsize, shuffle=False, pin_memory=True)\n",
        "images, labels = next(iter(dataloader))\n",
        "images, labels = images.cuda(), labels.cuda()\n",
        "#helper.imshow(images[0], normalize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14607bc7dc50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 transforms.Normalize(mean=[0.48, 0.45, 0.40], std=[0.23, 0.23, 0.23])])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/Colab Notebooks/Covid_Images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mbsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    254\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    124\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    125\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab Notebooks/Covid_Images/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6es1OdQNiGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133d9ff5-3b2c-4b79-fc6c-2f8f85e33e64"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZrvUv9QNn6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6f3777-e11d-45ff-9735-26e6723e3081"
      },
      "source": [
        "#cifar10...32x32\n",
        "num_classes=len(torch.unique(labels))\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYu9dZ4GNr9C"
      },
      "source": [
        "trainlen=round(.8*len(dataset))\n",
        "vallen=round(0.1*len(dataset))\n",
        "testlen=len(dataset)-round(trainlen+vallen)\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [trainlen, vallen, testlen], generator=torch.Generator().manual_seed(42))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYFqCmnvNxZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "a4f0c685-808f-473e-a537-981e83a2bfa9"
      },
      "source": [
        "len(trainlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-94c2df93f31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOiZweAuN00Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "81f65f3d-6be3-4284-9f2b-80e07b51ff88"
      },
      "source": [
        "len(testlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c440556fac42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnBdFIitN3jd"
      },
      "source": [
        "len(vallen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPQGM2RaN6HO"
      },
      "source": [
        "from torch.utils.data import  DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=8, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=8, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrp2gj-UODkR"
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NviiihhBOGyG"
      },
      "source": [
        "len(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlm1JcN3OJqQ"
      },
      "source": [
        "len(val_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpjjn3kRONFS"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models, utils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(train_loader))\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46V-IYXiOUvt"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# #pretrained=True will download a pretrained network for us\n",
        "# model = models.densenet201(pretrained=True)\n",
        "# model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwCjuIDcOZur"
      },
      "source": [
        "###Rwightman Models\n",
        "# !pip install timm\n",
        "# import timm\n",
        "# import torch\n",
        "# from torch.autograd import Variable\n",
        "# model = timm.create_model('resnet18', pretrained=True)\n",
        "# model \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4585wfQmOdOB"
      },
      "source": [
        "#timm.list_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsS2J2X8Ogkq"
      },
      "source": [
        "!pip install pytorchcv torch>=0.4.0\n",
        "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "model = ptcv_get_model(\"regnetx002\", pretrained=True)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzDiKUkjOklK"
      },
      "source": [
        "#Freezing model parameters and defining the fully connected network to be attached to the model, loss function and the optimizer.\n",
        "#We there after put the model on the GPUs\n",
        "#num_ftrs = model.output.fc3.in_features\n",
        "#num_ftrs = model.output.fc.in_features\n",
        "num_ftrs = 368\n",
        "#num_ftrs = model.output.in_features\n",
        "for param in model.parameters():\n",
        "  param.require_grad = False\n",
        "fc = nn.Sequential(\n",
        "    # nn.Linear(1920, 1920),\n",
        "    # nn.ReLU(),\n",
        "    # nn.Dropout(0.4),\n",
        "    \n",
        "    nn.Linear(num_ftrs,num_classes),\n",
        "    #nn.Dropout(0.6),\n",
        "    #nn.LogSoftmax(dim=1)\n",
        "    \n",
        ")\n",
        "model.output = fc\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Over here we want to only update the parameters of the classifier so\n",
        "optimizer = torch.optim.SGD(model.output.parameters(), lr=0.003)\n",
        "#optimizer = torch.optim.Adam(model.output.parameters(), lr = 0.003)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9qyUX8bOqt4"
      },
      "source": [
        "num_epochs = 15\n",
        "valid_loader = val_loader\n",
        "# keeping-track-of-losses \n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # keep-track-of-training-and-validation-loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    correct = 0\n",
        "    total=0\n",
        "    # training-the-model\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move-tensors-to-GPU \n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        # clear-the-gradients-of-all-optimized-variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
        "        output = model(data)\n",
        "        # calculate-the-batch-loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n",
        "        loss.backward()\n",
        "        # perform-a-ingle-optimization-step (parameter-update)\n",
        "        optimizer.step()\n",
        "        # update-training-loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        ##TODAY\n",
        "        _,pred = torch.max(output, dim=1)\n",
        "        correct += torch.sum(pred==target).item()\n",
        "        total += target.size(0)\n",
        "    train_acc.append(100 * correct / total)\n",
        "    # validate-the-model\n",
        "    model.eval()\n",
        "    total_t=0\n",
        "    correct_t=0\n",
        "    for data, target in valid_loader:\n",
        "        \n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        \n",
        "        output = model(data)\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # update-average-validation-loss \n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "        ###today\n",
        "        _,pred_t = torch.max(output, dim=1)\n",
        "        correct_t += torch.sum(pred_t==target).item()\n",
        "        total_t += target.size(0)\n",
        "    val_acc.append(100 * correct_t / total_t)\n",
        "    # calculate-average-losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "        \n",
        "    # print-training/validation-statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyWoRwTDOxEP"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-A_oJjgO07K"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.plot(train_acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ4_JyN2O4UH"
      },
      "source": [
        "# test-the-model\n",
        "model.eval()  # it-disables-dropout\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "          \n",
        "    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save \n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hSwAAxVO945"
      },
      "source": [
        "# def train_model(train_dl, model):\n",
        "#     # define the optimization\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "# #Over here we want to only update the parameters of the classifier so\n",
        "#     optimizer = torch.optim.SGD(model.output.parameters(), lr=0.003)\n",
        "#     # enumerate epochs\n",
        "#     losses = []\n",
        "#     running_corrects = 0\n",
        "#     epochacc = []\n",
        "#     for epoch in range(10):\n",
        "#         # enumerate mini batches\n",
        "#         running_loss = 0.0\n",
        "#         for i, (inputs, targets) in enumerate(train_dl):\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             # clear the gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # compute the model output\n",
        "#             yhat = model(inputs)\n",
        "#             # calculate loss\n",
        "#             loss = criterion(yhat, targets)\n",
        "#             _, preds = torch.max(yhat, 1)\n",
        "#             # credit assignment\n",
        "#             loss.backward()\n",
        "#             # update model weights\n",
        "#             optimizer.step()\n",
        "#             running_loss += loss.item() * inputs.size(0)\n",
        "#             running_corrects += torch.sum(preds == targets.data)\n",
        "#         epoch_loss = running_loss / len(train_loader)\n",
        "#         epoch_acc = running_corrects.double() / len(train_loader)\n",
        "#         losses.append(epoch_loss)\n",
        "#         epoch_acc.tolist()\n",
        "#         epochacc.append(epoch_acc)\n",
        "#         running_corrects.append(epoch_acc)\n",
        "#         print(epoch_acc) \n",
        "#     return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_lJPqnPEl3"
      },
      "source": [
        "# from numpy import argmax\n",
        "# from numpy import vstack\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# def evaluate_model(test_dl, model):\n",
        "#     predictions, actuals = list(), list()\n",
        "#     for i, (inputs, targets) in enumerate(test_dl):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         # evaluate the model on the test set\n",
        "#         yhat = model(inputs)\n",
        "#         # retrieve numpy array\n",
        "#         yhat = yhat.cpu().detach().numpy()\n",
        "#         actual = targets.cpu().numpy()\n",
        "#         # convert to class labels\n",
        "#         yhat = argmax(yhat, axis=1)\n",
        "#         # reshape for stacking\n",
        "#         actual = actual.reshape((len(actual), 1))\n",
        "#         yhat = yhat.reshape((len(yhat), 1))\n",
        "#         # store\n",
        "#         predictions.append(yhat)\n",
        "#         actuals.append(actual)\n",
        "#     predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "#     # calculate accuracy\n",
        "#     acc = accuracy_score(actuals, predictions)\n",
        "#     return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT6P8FGjPIo_"
      },
      "source": [
        "#losses = train_model(train_loader, model)\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1tL_YrKPOme"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(losses, label='TrainLoss', marker='o')\n",
        "# #plt.plot(epochacc, label='TrainAcc', marker='x')\n",
        "#   # plt.plot(x, history['val_loss'], label='val', marker='o')\n",
        "#     # plt.plot(x, history['train_loss'], label='train', marker='o')\n",
        "# plt.title('Loss per epoch'); plt.ylabel('Loss');\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(), plt.grid()\n",
        "# plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PotO_3ynPW4K"
      },
      "source": [
        "# trainacc = evaluate_model(train_loader, model)\n",
        "# print('TrainAccuracy: %.3f' % trainacc)\n",
        "# valacc = evaluate_model(val_loader, model)\n",
        "# print('ValAccuracy: %.3f' % valacc)\n",
        "# testacc = evaluate_model(test_loader, model)\n",
        "# print('Accuracy: %.3f' % testacc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}